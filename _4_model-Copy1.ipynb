{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packages import *\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score, r2_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from collections import Counter\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "We will be building our models in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some configuration required for tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-7\n",
    "fp = 'float32'\n",
    "tf.config.gpu.set_per_process_memory_growth(True)\n",
    "tf.config.gpu.set_per_process_memory_fraction(1.)\n",
    "tf.keras.backend.set_floatx(fp)\n",
    "tf.keras.backend.set_epsilon(epsilon)\n",
    "tf.keras.backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "glove_dim = 50\n",
    "input_shape = (300, glove_dim, 1)\n",
    "max_length = 300\n",
    "shuffle_buffer_size = batch_size*4\n",
    "prefetch_buffer_size = 1\n",
    "random_seed = np.random.randint(0, 100)\n",
    "test_ratio = 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_sample(dictionary, n_sample=None, random_seed=42):\n",
    "    lens = [len(l) for l in dictionary.values()]\n",
    "    assert min(lens) == max(lens)\n",
    "    n_data = lens[0]\n",
    "    processed = {}\n",
    "    for key, array in dictionary.items():\n",
    "        if n_sample is not None:\n",
    "            processed[key] = shuffle(array, random_state=random_seed)[:n_sample]\n",
    "        else:\n",
    "            processed[key] = shuffle(array, random_state=random_seed)[:n_sample]\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = shuffle_sample(load_data({'review': ['text', 'stars']})['review'],\n",
    "                             random_seed=random_seed\n",
    "                            )\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['text'],\n",
    "                                                    data['stars'],\n",
    "                                                    test_size = test_ratio,\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_lookup = glove2dict(os.path.join(GLOVE_DIR, 'glove.6B.{}d.txt'.format(glove_dim)))\n",
    "unk_key = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_vector = np.mean(np.array(list(glove_lookup.values())), axis=0)\n",
    "glove_lookup[unk_key] = unk_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_text = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "test_dataset_text = tf.data.Dataset.from_tensor_slices(x_test)\n",
    "\n",
    "train_dataset_stars = tf.data.Dataset.from_tensor_slices(y_train - 1) # to make stars 0-indexed\n",
    "test_dataset_stars = tf.data.Dataset.from_tensor_slices(y_test - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_token(t):\n",
    "    t = t.lower()\n",
    "    m = re.match('^[^\\w\\'](\\w+).*', t)\n",
    "    if m is not None:\n",
    "        return m.group(1)\n",
    "    else:\n",
    "        return t\n",
    "\n",
    "def tokenize(s):\n",
    "    tokens = []\n",
    "    for t in word_tokenize(s):\n",
    "        tokens.append(clean_token(t))\n",
    "    return tokens\n",
    "\n",
    "# Returns an np.array of glove embeddings for each\n",
    "# word in the given string of shape (word_count, glove_dims)\n",
    "def get_glove_embeddings(tokens):\n",
    "    embeddings = []\n",
    "    \n",
    "    for i, word in enumerate(tokens):\n",
    "#         if i > \n",
    "        if word in glove_lookup:\n",
    "            embeddings.append(glove_lookup[word])\n",
    "        else:\n",
    "            embeddings.append(np.zeros(glove_dim))\n",
    "            \n",
    "#     return(np.array(embeddings, dtype=np.float16))\n",
    "    return(np.array(embeddings, dtype=np.float32))\n",
    "\n",
    "def test_embed(arr):\n",
    "    return get_glove_embeddings(tokenize(arr))\n",
    "\n",
    "def embed(tensor):\n",
    "    return get_glove_embeddings(tokenize(str(tensor.numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.5 s, sys: 0 ns, total: 13.5 s\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(128 * 100):\n",
    "    _=test_embed(x_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def fix_dimensions(tensor):\n",
    "    return tf.image.resize_image_with_crop_or_pad(tf.expand_dims(tensor, axis=-1), 300, glove_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_embed = tf.data.Dataset.map(train_dataset_text,  lambda review: tf.py_function( embed, [review], tf.float32 ), num_parallel_calls=AUTOTUNE) \n",
    "test_dataset_embed = tf.data.Dataset.map(test_dataset_text,  lambda review: tf.py_function( embed, [review], tf.float32 ),num_parallel_calls=AUTOTUNE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_embed = tf.data.Dataset.map(train_dataset_embed, fix_dimensions, num_parallel_calls=AUTOTUNE) \n",
    "test_dataset_embed = tf.data.Dataset.map(test_dataset_embed, fix_dimensions, num_parallel_calls=AUTOTUNE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.zip((train_dataset_embed, train_dataset_stars))\n",
    "test_dataset = tf.data.Dataset.zip((test_dataset_embed, test_dataset_stars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.shuffle(train_dataset, buffer_size=shuffle_buffer_size)\n",
    "test_dataset = tf.data.Dataset.shuffle(test_dataset, buffer_size=shuffle_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.batch(train_dataset, batch_size=batch_size)\n",
    "test_dataset = tf.data.Dataset.batch(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.prefetch(train_dataset, buffer_size=prefetch_buffer_size)\n",
    "test_dataset = tf.data.Dataset.prefetch(test_dataset, buffer_size=prefetch_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_dataset:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tyler's Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_maxent_classifier(X, y):   \n",
    "    mod = LogisticRegression(\n",
    "        fit_intercept=True, \n",
    "        solver='liblinear', \n",
    "        multi_class='auto')\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-29-5d3daac8ad0e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-5d3daac8ad0e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    test_dataset_tyler = train_dataset_embed.map(lambda review, print(review))\u001b[0m\n\u001b[0m                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "test_dataset_tyler = train_dataset_embed.map(lambda review, print(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'ParallelMapDataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-25a909fe08fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit_maxent_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset_stars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-afc49789cf1e>\u001b[0m in \u001b[0;36mfit_maxent_classifier\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         multi_class='auto')\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1532\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1533\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    720\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'ParallelMapDataset'"
     ]
    }
   ],
   "source": [
    "fit_maxent_classifier(train_dataset_embed, test_dataset_stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel:\n",
    "    def __init__(self):\n",
    "        self.model = LinearSVC()\n",
    "        self.vectorizer = CountVectorizer(max_features = 400000) #number of entries in GLoVe\n",
    "        \n",
    "    def train(self, reviews, stars):\n",
    "        x = self.vectorizer.fit_transform(reviews)\n",
    "        self.model.fit(x, stars)\n",
    "\n",
    "    def predict(self, reviews):\n",
    "        x = self.vectorizer.transform(reviews)\n",
    "        predictions = self.model.predict(x)\n",
    "        return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "monster = BaselineModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monster.train(x_train[:n_train], y_train[:n_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = monster.predict(x_test[:n_train])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "f1_score(y_test[:n_train], predictions, average='macro')\n",
    "0.43700495451651733"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "f1_score(y_test[:n_train], predictions, average='micro')\n",
    "0.612856"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BabyBlueberry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BabyBlueberry:\n",
    "\n",
    "    def build(input_shape = input_shape, star_units=5):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = Conv2D(16, (3, 3), activation='relu', name='block1_conv1', padding='same')(inputs)\n",
    "        x = Conv2D(16, (3, 3), activation='relu', name='block1_conv2', padding='same')(x)\n",
    "        x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "        x = Conv2D(32, (3, 3), activation='relu', name='block2_conv1', padding='same')(x)\n",
    "        x = Conv2D(32, (3, 3), activation='relu', name='block2_conv2', padding='same')(x)\n",
    "        x = Conv2D(32, (3, 3), activation='relu', name='block2_conv3', padding='same')(x)\n",
    "        x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "        x = Conv2D(64, (3, 3), activation='relu', name='block3_conv1', padding='same')(x)\n",
    "        x = Conv2D(64, (3, 3), activation='relu', name='block3_conv2', padding='same')(x)\n",
    "        x = Conv2D(64, (3, 3), activation='relu', name='block3_conv3', padding='same')(x)\n",
    "        x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x) \n",
    "        x = Conv2D(128, (3, 3), activation='relu', name='block4_conv1', padding='same')(x)\n",
    "        x = Conv2D(128, (3, 3), activation='relu', name='block4_conv2', padding='same')(x)\n",
    "        x = Conv2D(128, (3, 3), activation='relu', name='block4_conv3', padding='same')(x)\n",
    "        x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x) \n",
    "        x = Flatten(name='flatten')(x)\n",
    "        x = Dropout(rate=.2, name='dropout')(x)\n",
    "        out = Dense(star_units, activation='softmax', name='output')(x)\n",
    "        return tf.keras.models.Model(inputs=inputs, outputs=out)\n",
    "        \n",
    "    @tf.function\n",
    "    def loss_fn(truth, logits):\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(truth, logits)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.7 s, sys: 19.7 s, total: 1min 16s\n",
      "Wall time: 42.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in train_dataset.take(100):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "baby = BabyBlueberry.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "baby.compile(optimizer=adam, metrics=['accuracy'] ,loss=BabyBlueberry.loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "34997/34997 [==============================] - 14241s 407ms/step - loss: 0.8946 - accuracy: 0.1770\n",
      "Epoch 2/2\n",
      "34997/34997 [==============================] - 14242s 407ms/step - loss: 0.8171 - accuracy: 0.1790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f34fbf2ffd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baby.fit(train_dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 410s 410ms/step - loss: 0.8017 - accuracy: 0.1655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8017106358408927, 0.16546875]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baby.evaluate(test_dataset.take(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 6s 577ms/step - loss: 0.7471 - accuracy: 0.1750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7470987379550934, 0.175]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baby.evaluate(train_dataset.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dists, ix = neighbors(glove_lookup['female'] + glove_lookup['king'], glove_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dists[dists.columns[ix[::-1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yelp-nlu_3.6",
   "language": "python",
   "name": "yelp-nlu_3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
