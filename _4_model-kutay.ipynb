{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packages import *\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score, r2_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-alpha0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-7\n",
    "fp = 'float32'\n",
    "tf.config.gpu.set_per_process_memory_growth(True)\n",
    "tf.config.gpu.set_per_process_memory_fraction(.4)\n",
    "tf.keras.backend.set_floatx(fp)\n",
    "tf.keras.backend.set_epsilon(epsilon)\n",
    "tf.keras.backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "glove_dim = 50\n",
    "input_shape = (300, glove_dim, 1)\n",
    "max_length = 300\n",
    "shuffle_buffer_size = batch_size*4\n",
    "prefetch_buffer_size = 1\n",
    "random_seed = np.random.randint(0, 100)\n",
    "test_ratio = 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_sample(dictionary, n_sample=None, random_seed=42):\n",
    "    lens = [len(l) for l in dictionary.values()]\n",
    "    assert min(lens) == max(lens)\n",
    "    n_data = lens[0]\n",
    "    processed = {}\n",
    "    for key, array in dictionary.items():\n",
    "        if n_sample is not None:\n",
    "            processed[key] = shuffle(array, random_state=random_seed)[:n_sample]\n",
    "        else:\n",
    "            processed[key] = shuffle(array, random_state=random_seed)[:n_sample]\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = shuffle_sample(load_data({'review': ['text', 'stars']})['review'],\n",
    "                             random_seed=random_seed\n",
    "                            )\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['text'],\n",
    "                                                    data['stars'],\n",
    "                                                    test_size = test_ratio,\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_lookup = load_pickle(os.path.join(GLOVE_DIR, 'glove-{}D.pkl'.format(glove_dim)))\n",
    "unk_key = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_vector = np.mean(np.array(list(glove_lookup.values())), axis=0)\n",
    "glove_lookup[unk_key] = unk_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(glove_lookup.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "4479553 train sequences\n",
      "2206347 test sequences\n",
      "Average train sequence length: 602\n",
      "Average test sequence length: 602\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "# (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train_tf = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "y_train_tf = tf.data.Dataset.from_tensor_slices(y_train - 1) # to make stars 0-indexed\n",
    "\n",
    "x_test_tf = tf.data.Dataset.from_tensor_slices(x_test)\n",
    "y_test_tf = tf.data.Dataset.from_tensor_slices(y_test - 1)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Average train sequence length: {}'.format(\n",
    "    np.mean(list(map(len, x_train)), dtype=int)))\n",
    "print('Average test sequence length: {}'.format(\n",
    "    np.mean(list(map(len, x_test)), dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_token(t):\n",
    "    t = t.lower()\n",
    "    m = re.match('^[^\\w\\'](\\w+).*', t)\n",
    "    if m is not None:\n",
    "        return m.group(1)\n",
    "    else:\n",
    "        return t\n",
    "\n",
    "def tokenize(s):\n",
    "    tokens = []\n",
    "    for t in word_tokenize(s):\n",
    "        tokens.append(clean_token(t))\n",
    "    return tokens\n",
    "\n",
    "# Returns an np.array of glove embeddings for each\n",
    "# word in the given string of shape (word_count, glove_dims)\n",
    "def get_glove_embeddings(tokens):\n",
    "    embeddings = []\n",
    "    \n",
    "    for i, word in enumerate(tokens):\n",
    "#         if i > \n",
    "        if word in glove_lookup:\n",
    "            embeddings.append(glove_lookup[word])\n",
    "        else:\n",
    "            embeddings.append(np.zeros(glove_dim))\n",
    "            \n",
    "#     return(np.array(embeddings, dtype=np.float16))\n",
    "    return(np.array(embeddings, dtype=np.float32))\n",
    "\n",
    "def test_embed(arr):\n",
    "    return get_glove_embeddings(tokenize(arr))\n",
    "\n",
    "def embed(tensor):\n",
    "    return get_glove_embeddings(tokenize(str(tensor.numpy())))\n",
    "\n",
    "@tf.function\n",
    "def fix_dimensions(tensor):\n",
    "    return tf.reshape(tf.image.resize_image_with_crop_or_pad(tf.expand_dims(tensor, axis=-1), 300, glove_dim), (300,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_embed = tf.data.Dataset.map(x_train_tf,  lambda review: tf.py_function( embed, [review], tf.float32 ), num_parallel_calls=AUTOTUNE) \n",
    "test_dataset_embed = tf.data.Dataset.map(x_test_tf,  lambda review: tf.py_function( embed, [review], tf.float32 ),num_parallel_calls=AUTOTUNE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_embed = tf.data.Dataset.map(train_dataset_embed, fix_dimensions, num_parallel_calls=AUTOTUNE) \n",
    "test_dataset_embed = tf.data.Dataset.map(test_dataset_embed, fix_dimensions, num_parallel_calls=AUTOTUNE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.zip((train_dataset_embed, y_train_tf))\n",
    "test_dataset = tf.data.Dataset.zip((test_dataset_embed, y_test_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.shuffle(train_dataset, buffer_size=shuffle_buffer_size)\n",
    "test_dataset = tf.data.Dataset.shuffle(test_dataset, buffer_size=shuffle_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.batch(train_dataset, batch_size=batch_size)\n",
    "test_dataset = tf.data.Dataset.batch(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.prefetch(train_dataset, buffer_size=prefetch_buffer_size)\n",
    "test_dataset = tf.data.Dataset.prefetch(test_dataset, buffer_size=prefetch_buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step #1: \n",
    "(# of obsv, # of words, # of glove dims, ...)\n",
    "(16, 300, 50, 1)\n",
    "Access an observation:\n",
    "X[3].numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get data from tensor\n",
    "X = []\n",
    "Y = []\n",
    "for x_batch, y_batch in train_dataset.take(10000 // batch_size):\n",
    "    X.append(x_batch)\n",
    "    Y.append(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 200\n",
    "input_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpLSTM:\n",
    "    def __init__(self, hidden_size=5, output_dim=5):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "    def build(self):\n",
    "        inp = Input(shape=(input_len, glove_dim), name=\"input\")\n",
    "        #x = Embedding(2500, embed_dim, input_length = input_len)(inp)\n",
    "        #x = Dropout(0.2)(x)\n",
    "        x = LSTM(self.hidden_size)(inp)\n",
    "        out = Dense(self.output_dim, activation=\"softmax\")(x)\n",
    "        return tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "        \n",
    "    @tf.function\n",
    "    def loss_fn(truth, logits):\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(truth, logits)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:04:54.084853 140203855951680 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f82d46a4ac8>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "200/200 [==============================] - 132s 659ms/step - loss: 1.4492 - accuracy: 0.0301 - val_loss: 1.3891 - val_accuracy: 0.0351\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 131s 655ms/step - loss: 1.3631 - accuracy: 0.0490 - val_loss: 1.3266 - val_accuracy: 0.0635\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 131s 654ms/step - loss: 1.2979 - accuracy: 0.2613 - val_loss: 1.2613 - val_accuracy: 0.3629\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 131s 656ms/step - loss: 1.2674 - accuracy: 0.3948 - val_loss: 1.2364 - val_accuracy: 0.3607\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 131s 655ms/step - loss: 1.2422 - accuracy: 0.3848 - val_loss: 1.2198 - val_accuracy: 0.3327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f82d4dc29b0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YelpLSTM(hidden_size=10)\n",
    "model = model.build()\n",
    "model.compile(optimizer=\"adam\", metrics=['accuracy'] ,loss=YelpLSTM.loss_fn)\n",
    "model.fit(train_dataset.take(200),\n",
    "          epochs=5,\n",
    "          #validation_split=.2,\n",
    "          validation_data=test_dataset.take(200), validation_freq=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:15:50.630335 140203855951680 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f82d42d95f8>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "200/200 [==============================] - 132s 658ms/step - loss: 1.4473 - accuracy: 0.0510 - val_loss: 1.3940 - val_accuracy: 0.0432\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 131s 655ms/step - loss: 1.3280 - accuracy: 0.2346 - val_loss: 1.2935 - val_accuracy: 0.3620\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 131s 655ms/step - loss: 1.2814 - accuracy: 0.3286 - val_loss: 1.2378 - val_accuracy: 0.3742\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 131s 655ms/step - loss: 1.1899 - accuracy: 0.2669 - val_loss: 1.1775 - val_accuracy: 0.2939\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 131s 656ms/step - loss: 1.1510 - accuracy: 0.2436 - val_loss: 1.1536 - val_accuracy: 0.2692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f82d4e3cd68>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YelpLSTM(hidden_size=20)\n",
    "model = model.build()\n",
    "model.compile(optimizer=\"adam\", metrics=['accuracy'] ,loss=YelpLSTM.loss_fn)\n",
    "model.fit(train_dataset.take(200),\n",
    "          epochs=5,\n",
    "          #validation_split=.2,\n",
    "          validation_data=test_dataset.take(200), validation_freq=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:26:47.036465 140203855951680 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f82d42b8630>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "200/200 [==============================] - 132s 660ms/step - loss: 1.4282 - accuracy: 0.0435 - val_loss: 1.3925 - val_accuracy: 0.0507\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 132s 659ms/step - loss: 1.4093 - accuracy: 0.0227 - val_loss: 1.4122 - val_accuracy: 0.0334\n",
      "Epoch 3/5\n",
      "199/200 [============================>.] - ETA: 0s - loss: 1.4134 - accuracy: 0.0461"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-c35f71771930>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0;31m#validation_split=.2,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m          )\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    789\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m           initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[0;31m# Case 3: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m           \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m           steps_name='validation_steps')\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(generator, mode)\u001b[0m\n\u001b[1;32m    353\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \"\"\"\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m         \u001b[0;34m\"IteratorGetNextSync\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m         \"output_types\", output_types, \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   1941\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = YelpLSTM(hidden_size=50)\n",
    "model = model.build()\n",
    "model.compile(optimizer=\"adam\", metrics=['accuracy'] ,loss=YelpLSTM.loss_fn)\n",
    "model.fit(train_dataset.take(200),\n",
    "          epochs=5,\n",
    "          #validation_split=.2,\n",
    "          validation_data=test_dataset.take(200), validation_freq=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpLSTM2:\n",
    "    def __init__(self, hidden_size=20, dense_layer_dim=100, output_dim=5):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_dim = output_dim\n",
    "        self.dense_layer_dim = dense_layer_dim\n",
    "    \n",
    "    def build(self):\n",
    "        inp = Input(shape=(input_len, glove_dim), name=\"input\")\n",
    "        #x = Embedding(2500, embed_dim, input_length = input_len)(inp)\n",
    "        x = LSTM(self.hidden_size)(inp)\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = Dense(self.dense_layer_dim, activation=\"relu\")(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        out = Dense(self.output_dim, activation=\"softmax\")(x)\n",
    "        return tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "        \n",
    "    @tf.function\n",
    "    def loss_fn(truth, logits):\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(truth, logits)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0605 23:35:13.953759 139622214588224 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7efb511f9c18>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "200/200 [==============================] - 134s 672ms/step - loss: 1.4409 - accuracy: 0.0382 - val_loss: 1.4100 - val_accuracy: 0.0572\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 131s 657ms/step - loss: 1.3791 - accuracy: 0.0806 - val_loss: 1.3315 - val_accuracy: 0.1919\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 132s 658ms/step - loss: 1.2916 - accuracy: 0.2806 - val_loss: 1.2570 - val_accuracy: 0.3425\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 131s 657ms/step - loss: 1.2068 - accuracy: 0.2849 - val_loss: 1.1715 - val_accuracy: 0.2089\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 131s 656ms/step - loss: 1.1351 - accuracy: 0.2330 - val_loss: 1.1436 - val_accuracy: 0.1609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efb512f3cf8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YelpLSTM2(hidden_size=20, dense_layer_dim=100)\n",
    "model = model.build()\n",
    "model.compile(optimizer=\"adam\", metrics=['accuracy'] ,loss=YelpLSTM.loss_fn)\n",
    "model.fit(train_dataset.take(200),\n",
    "          epochs=5,\n",
    "          #validation_split=.2,\n",
    "          validation_data=test_dataset.take(200), validation_freq=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 300, 50)]         0         \n",
      "_________________________________________________________________\n",
      "unified_lstm (UnifiedLSTM)   (None, 20)                5680      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               2100      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 8,285\n",
      "Trainable params: 8,285\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Feature Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n",
    "        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(\"{0}:{1}\".format(\n",
    "        token.text, token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexed Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packages import *\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "from sklearn.metrics import f1_score, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.gpu.set_per_process_memory_growth(True)\n",
    "tf.config.gpu.set_per_process_memory_fraction(.40)\n",
    "tf.keras.backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "glove_dim = 300\n",
    "input_len = 300\n",
    "input_shape = (300, glove_dim, 1)\n",
    "max_length = 300\n",
    "shuffle_buffer_size = batch_size * 10\n",
    "prefetch_buffer_size = 1\n",
    "chunk_count = 100\n",
    "random_seed = np.random.randint(0, 1000)\n",
    "splits = ['train', 'val', 'test'] #80:10:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_text_filenames = [os.path.join(DATASET_DIR, 'preprocessed', 'tfrecord', 'xext',  #xext temporarily\n",
    "                                  'review-text-{:02d}.tf'.format(i)) for i in range(chunk_count)]\n",
    "tf_ix_filenames = [os.path.join(DATASET_DIR, 'preprocessed', 'tfrecord', 'ix', \n",
    "                                  'review-ix-{:02d}.tf'.format(i)) for i in range(chunk_count)]\n",
    "stars = (load_data({'review': ['stars']})['review']['stars'] - 1).astype(np.int32)# this is so that stars are 0-indexed\n",
    "N = len(stars)\n",
    "stars_chunked = stars.reshape((chunk_count, 6685900//chunk_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_text, tf_ix, stars= {}, {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_text['train'], tf_text_filenames_val_test, tf_ix['train'], tf_ix_filenames_val_test, stars['train'], stars_val_test \\\n",
    "= train_test_split(tf_text_filenames, tf_ix_filenames, stars_chunked, random_state=random_seed, test_size = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_text['val'], tf_text['test'], tf_ix['val'], tf_ix['test'], stars['val'], stars['test'] \\\n",
    "= train_test_split(tf_text_filenames_val_test, tf_ix_filenames_val_test, stars_val_test, random_state=random_seed, test_size = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in splits:\n",
    "    stars[split] = stars[split].reshape(np.product(stars[split].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80:10:10)\n"
     ]
    }
   ],
   "source": [
    "print('({}:{}:{})'.format(len(tf_text['train']), len(tf_text['val']), len(tf_text['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_lookup = load_pickle(os.path.join(GLOVE_DIR, 'glove-{}D-byte-float32.pkl'.format(glove_dim)))\n",
    "keys_to_ix = load_pickle(os.path.join(GLOVE_DIR, 'glove-byte-keys_to_ix.pkl'))\n",
    "ix_to_key = {value: key for key, value in keys_to_ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_vector = np.mean(np.array(list(glove_lookup.values())), axis=0)\n",
    "null_vector = np.zeros(glove_dim)\n",
    "glove_lookup[UNK_KEY.encode('ascii')] = unk_vector #if using byte glove dict\n",
    "glove_lookup[NULL_KEY.encode('ascii')] = null_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oops 1 times.\n"
     ]
    }
   ],
   "source": [
    "oops = 0\n",
    "glove_lookup_array = []\n",
    "for i in range(len(ix_to_key)):\n",
    "    if ix_to_key[i] not in glove_lookup:\n",
    "        oops+=1\n",
    "    glove_lookup_array.append(glove_lookup.get(ix_to_key[i], null_vector))\n",
    "glove_lookup_array = np.array(glove_lookup_array, dtype=np.float32)\n",
    "print('oops {} times.'.format(oops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def _parse_function(proto, to_ix):\n",
    "    # define your tfrecord again. Remember that you saved your image as a string.\n",
    "    keys_to_features = {'review': tf.io.FixedLenFeature([300,], tf.int64) if to_ix else tf.io.FixedLenFeature([300,], tf.string),}\n",
    "    \n",
    "    # Load one example\n",
    "    parsed_features = tf.io.parse_single_example(proto, keys_to_features)\n",
    "    \n",
    "    return parsed_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_embedding = tf.Variable(\n",
    "#         tf.constant(glove_lookup_array),\n",
    "#         trainable=False,\n",
    "#         name=\"Embedding\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def embed(tensor):\n",
    "    return tf.gather(glove_lookup_array, tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def channelize(tensor):\n",
    "    return tf.expand_dims(tensor, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lstm_dataset(embedded, to_ix=False):        \n",
    "    if embedded or to_ix: \n",
    "        files = tf_ix\n",
    "        to_ix = True\n",
    "    else:\n",
    "        files = tf_text\n",
    "    dataset = {}\n",
    "    for split in splits:\n",
    "        dataset[split] = tf.data.TFRecordDataset(files[split])\n",
    "        stars_dataset = tf.data.Dataset.from_tensor_slices(stars[split])\n",
    "        dataset[split] = dataset[split].map(lambda x: _parse_function(x, to_ix)['review'], num_parallel_calls=AUTOTUNE)\n",
    "        if embedded:\n",
    "            dataset[split] = dataset[split].map(embed, num_parallel_calls=AUTOTUNE)\n",
    "            #dataset[split] = dataset[split].map(channelize, num_parallel_calls=AUTOTUNE)\n",
    "        dataset[split] = tf.data.Dataset.zip((dataset[split], stars_dataset))\n",
    "        dataset[split] = dataset[split].shuffle(shuffle_buffer_size)\n",
    "        dataset[split] = dataset[split].map(lambda x, y: ({'review': x}, {'stars': y}))\n",
    "        dataset[split] = dataset[split].batch(batch_size)\n",
    "        dataset[split] = dataset[split].prefetch(prefetch_buffer_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = make_lstm_dataset(embedded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check(model, dataset):\n",
    "    print(\"TRAIN:\")\n",
    "    for i in dataset['train']:\n",
    "        break\n",
    "    print(i[1])\n",
    "    print(np.argmax(model.predict_on_batch(i[0]['review'])['stars'], axis=1))\n",
    "    print(\"TEST:\")\n",
    "    for i in dataset['test']:\n",
    "        break\n",
    "    print(i[1])\n",
    "    print(np.argmax(model.predict_on_batch(i[0]['review'])['stars'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Levreki:\n",
    "    def __init__(self, lstm_size=5, output_dim=5, input_shape=(300,50)):\n",
    "        self.lstm_size = lstm_size\n",
    "        self.output_dim = output_dim\n",
    "        self.input_shape = input_shape\n",
    "    \n",
    "    def build(self):\n",
    "        inputs = Input(shape=self.input_shape, name='review')\n",
    "        #x = Embedding(2500, embed_dim, input_length = input_len)(inp)\n",
    "        #x = Dropout(0.2)(x)\n",
    "        x = LSTM(self.lstm_size, name=\"LSTM\")(inputs)\n",
    "        outputs = Dense(self.output_dim, activation=\"softmax\", name=\"stars\")(x)\n",
    "        return tf.keras.models.Model(inputs={'review' : inputs}, outputs={'stars': outputs}, name='Levreki')\n",
    "    \n",
    "    def get_params(self):\n",
    "        return {\n",
    "            'lstm_size' : self.lstm_size\n",
    "        }\n",
    "\n",
    "    def get_param_string(self):\n",
    "        return \"{}_{}cells\".format(self.name, self.lstm_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tilapia:\n",
    "    def __init__(self, lstm_size=5, dense_size=1000, output_dim=5, input_shape=(300,50)):\n",
    "        self.lstm_size = lstm_size\n",
    "        self.output_dim = output_dim\n",
    "        self.dense_size = dense_size\n",
    "        self.input_shape = input_shape\n",
    "    \n",
    "    def build(self):\n",
    "        inputs = Input(shape=self.input_shape, name='review')\n",
    "        #x = Dropout(0.2)(x)\n",
    "        x = LSTM(self.lstm_size, name=\"LSTM_1\")(inputs)\n",
    "        x = Dense(self.dense_size, name=\"dense_1\")(x)\n",
    "        outputs = Dense(self.output_dim, activation=\"softmax\", name=\"stars\")(x)\n",
    "        return tf.keras.models.Model(inputs={'review' : inputs}, outputs={'stars': outputs}, name='Tilapia')\n",
    "\n",
    "    def get_params(self):\n",
    "        return {\n",
    "            'lstm_size' : self.lstm_size,\n",
    "            'dense_size': self.dense_size\n",
    "        }\n",
    "\n",
    "    def get_param_string(self):\n",
    "        return \"{}_{}cells_{}dense\".format(self.name, self.lstm_size, self.dense_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_blueberry = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0606 13:55:30.268415 139888336217920 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f39f0b916d8>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Levreki\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "review (InputLayer)          [(None, 300, 300)]        0         \n",
      "_________________________________________________________________\n",
      "LSTM (UnifiedLSTM)           (None, 5)                 6120      \n",
      "_________________________________________________________________\n",
      "stars (Dense)                (None, 5)                 30        \n",
      "=================================================================\n",
      "Total params: 6,150\n",
      "Trainable params: 6,150\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 72s 72ms/step - loss: 1.4169 - sparse_categorical_accuracy: 0.4536 - val_loss: 0.0000e+00 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 70s 70ms/step - loss: 1.3304 - sparse_categorical_accuracy: 0.4957 - val_loss: 1.3127 - val_sparse_categorical_accuracy: 0.5134\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 71s 71ms/step - loss: 1.4309 - sparse_categorical_accuracy: 0.4231 - val_loss: 1.4113 - val_sparse_categorical_accuracy: 0.4407\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 75s 75ms/step - loss: 1.4030 - sparse_categorical_accuracy: 0.4442 - val_loss: 1.3862 - val_sparse_categorical_accuracy: 0.4463\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 71s 71ms/step - loss: 1.3602 - sparse_categorical_accuracy: 0.4504 - val_loss: 1.3281 - val_sparse_categorical_accuracy: 0.4540\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 71s 71ms/step - loss: 1.3930 - sparse_categorical_accuracy: 0.4486 - val_loss: 1.3932 - val_sparse_categorical_accuracy: 0.4480\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 84s 84ms/step - loss: 1.3526 - sparse_categorical_accuracy: 0.4512 - val_loss: 1.3253 - val_sparse_categorical_accuracy: 0.4532\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 83s 83ms/step - loss: 1.3028 - sparse_categorical_accuracy: 0.4805 - val_loss: 1.2767 - val_sparse_categorical_accuracy: 0.5058\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 71s 71ms/step - loss: 1.2837 - sparse_categorical_accuracy: 0.5127 - val_loss: 1.2637 - val_sparse_categorical_accuracy: 0.5219\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 71s 71ms/step - loss: 1.2690 - sparse_categorical_accuracy: 0.5015 - val_loss: 1.2612 - val_sparse_categorical_accuracy: 0.4922\n",
      "TRAIN:\n",
      "{'stars': <tf.Tensor: id=13083245, shape=(64,), dtype=int32, numpy=\n",
      "array([4, 4, 4, 2, 0, 1, 1, 0, 0, 1, 3, 1, 4, 3, 4, 4, 4, 1, 4, 0, 0, 2,\n",
      "       4, 3, 2, 3, 2, 4, 4, 3, 3, 2, 1, 4, 0, 2, 0, 3, 4, 1, 3, 3, 3, 0,\n",
      "       0, 3, 0, 4, 1, 4, 0, 3, 4, 4, 4, 4, 4, 4, 3, 3, 4, 4, 1, 3],\n",
      "      dtype=int32)>}\n",
      "[4 0 4 4 0 0 4 0 0 0 4 0 0 4 4 0 0 0 4 0 0 4 4 4 0 4 4 4 4 4 4 0 0 4 0 4 0\n",
      " 4 4 3 4 4 4 0 0 4 4 4 4 4 4 4 4 0 4 0 4 4 4 0 4 4 0 4]\n",
      "TEST:\n",
      "{'stars': <tf.Tensor: id=13083372, shape=(64,), dtype=int32, numpy=\n",
      "array([4, 3, 4, 4, 4, 4, 0, 4, 1, 4, 3, 4, 1, 2, 3, 3, 4, 0, 4, 3, 3, 2,\n",
      "       3, 4, 0, 4, 4, 4, 4, 3, 3, 1, 3, 4, 4, 3, 3, 2, 1, 4, 3, 3, 1, 4,\n",
      "       4, 1, 4, 0, 4, 0, 2, 2, 4, 4, 3, 4, 3, 3, 4, 3, 4, 4, 2, 4],\n",
      "      dtype=int32)>}\n",
      "[4 4 4 4 4 4 0 4 4 4 4 4 4 0 4 4 4 4 4 4 4 4 4 4 0 0 4 0 4 0 0 0 4 4 4 4 4\n",
      " 0 0 4 0 4 4 0 4 0 4 0 0 0 0 4 4 4 0 4 4 0 4 4 0 0 4 4]\n"
     ]
    }
   ],
   "source": [
    "levreki = Levreki(lstm_size=5, output_dim=5, input_shape=(input_len, glove_dim)).build()\n",
    "levreki.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "log_dir = os.path.join(SRC_DIR, 'logs', 'LSTM_5cell_{}'.format(datetime.fromtimestamp(time.time()).strftime('%H-%M-%S_%Y-%m-%d')))\n",
    "tb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=False, update_freq='batch')\n",
    "levreki.summary()\n",
    "levreki.fit(dataset['train'].take(1000),\n",
    "            epochs=num_epochs,\n",
    "            validation_data = dataset['val'].take(1000),\n",
    "            callbacks=[tb])\n",
    "sanity_check(levreki, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0606 14:26:05.382736 140323491501888 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f9f563f1358>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Levreki\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "review (InputLayer)          [(None, 300, 300)]        0         \n",
      "_________________________________________________________________\n",
      "LSTM (UnifiedLSTM)           (None, 10)                12440     \n",
      "_________________________________________________________________\n",
      "stars (Dense)                (None, 5)                 55        \n",
      "=================================================================\n",
      "Total params: 12,495\n",
      "Trainable params: 12,495\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "      1/Unknown - 2s 2s/step - loss: 1.6110 - sparse_categorical_accuracy: 0.0938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0606 14:26:09.510894 140323491501888 callbacks.py:236] Method (on_train_batch_end) is slow compared to the batch update (0.132141). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 84s 84ms/step - loss: 1.4221 - sparse_categorical_accuracy: 0.4368 - val_loss: 0.0000e+00 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 82s 82ms/step - loss: 1.3752 - sparse_categorical_accuracy: 0.4419 - val_loss: 1.2726 - val_sparse_categorical_accuracy: 0.4589\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 82s 82ms/step - loss: 1.4152 - sparse_categorical_accuracy: 0.4401 - val_loss: 1.3574 - val_sparse_categorical_accuracy: 0.4534\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 85s 85ms/step - loss: 1.3887 - sparse_categorical_accuracy: 0.4376 - val_loss: 1.4012 - val_sparse_categorical_accuracy: 0.4534\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 94s 94ms/step - loss: 1.3872 - sparse_categorical_accuracy: 0.4452 - val_loss: 1.3137 - val_sparse_categorical_accuracy: 0.4558\n",
      "TRAIN:\n",
      "{'stars': <tf.Tensor: id=2965997, shape=(64,), dtype=int32, numpy=\n",
      "array([2, 1, 4, 1, 4, 3, 4, 3, 1, 3, 4, 4, 3, 4, 4, 3, 3, 4, 4, 3, 4, 4,\n",
      "       4, 4, 2, 3, 4, 0, 0, 4, 3, 0, 4, 3, 4, 4, 3, 4, 2, 3, 4, 4, 4, 2,\n",
      "       4, 0, 2, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 1, 4, 0, 3, 3],\n",
      "      dtype=int32)>}\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "TEST:\n",
      "{'stars': <tf.Tensor: id=2966124, shape=(64,), dtype=int32, numpy=\n",
      "array([4, 0, 2, 4, 0, 2, 4, 4, 4, 4, 3, 4, 4, 4, 0, 4, 2, 4, 3, 4, 4, 4,\n",
      "       4, 3, 4, 3, 4, 4, 0, 0, 4, 1, 1, 4, 3, 3, 4, 0, 4, 0, 3, 3, 4, 1,\n",
      "       2, 3, 0, 3, 4, 0, 4, 0, 3, 2, 4, 3, 4, 4, 3, 4, 4, 0, 4, 4],\n",
      "      dtype=int32)>}\n",
      "[4 4 4 4 4 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 4 4 4 4 4 4 4 4 4 4 0 4 4 4 4\n",
      " 4 4 4 4 4 4 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 4 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "levreki_10cell = Levreki(lstm_size=10, output_dim=5, input_shape=(input_len, glove_dim)).build()\n",
    "levreki_10cell.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "log_dir = os.path.join(SRC_DIR, 'logs', 'LSTM_10cell_{}'.format(datetime.fromtimestamp(time.time()).strftime('%H-%M-%S_%Y-%m-%d')))\n",
    "tb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=False, update_freq='batch')\n",
    "levreki_10cell.summary()\n",
    "levreki_10cell.fit(dataset['train'].take(1000),\n",
    "                   epochs=num_epochs,\n",
    "                   validation_data = dataset['val'].take(1000),\n",
    "                  callbacks = [tb])\n",
    "sanity_check(levreki_10cell, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0606 14:51:22.128246 140323491501888 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f9f5631c860>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Tilapia\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "review (InputLayer)          [(None, 300, 300)]        0         \n",
      "_________________________________________________________________\n",
      "LSTM_1 (UnifiedLSTM)         (None, 5)                 6120      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               600       \n",
      "_________________________________________________________________\n",
      "stars (Dense)                (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 7,225\n",
      "Trainable params: 7,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 104s 104ms/step - loss: 1.4180 - sparse_categorical_accuracy: 0.4353 - val_loss: 0.0000e+00 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 105s 105ms/step - loss: 1.4209 - sparse_categorical_accuracy: 0.4395 - val_loss: 1.4095 - val_sparse_categorical_accuracy: 0.4504\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 105s 105ms/step - loss: 1.4160 - sparse_categorical_accuracy: 0.4424 - val_loss: 1.4035 - val_sparse_categorical_accuracy: 0.4540\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 105s 105ms/step - loss: 1.4111 - sparse_categorical_accuracy: 0.4420 - val_loss: 1.4176 - val_sparse_categorical_accuracy: 0.4421\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 105s 105ms/step - loss: 1.4230 - sparse_categorical_accuracy: 0.4362 - val_loss: 1.4138 - val_sparse_categorical_accuracy: 0.4449\n",
      "TRAIN:\n",
      "{'stars': <tf.Tensor: id=6107216, shape=(64,), dtype=int32, numpy=\n",
      "array([2, 1, 4, 1, 4, 3, 4, 3, 1, 3, 4, 4, 3, 4, 4, 3, 3, 4, 4, 3, 4, 4,\n",
      "       4, 4, 2, 3, 4, 0, 0, 4, 3, 0, 4, 3, 4, 4, 3, 4, 2, 3, 4, 4, 4, 2,\n",
      "       4, 0, 2, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 1, 4, 0, 3, 3],\n",
      "      dtype=int32)>}\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "TEST:\n",
      "{'stars': <tf.Tensor: id=6107347, shape=(64,), dtype=int32, numpy=\n",
      "array([4, 0, 2, 4, 0, 2, 4, 4, 4, 4, 3, 4, 4, 4, 0, 4, 2, 4, 3, 4, 4, 4,\n",
      "       4, 3, 4, 3, 4, 4, 0, 0, 4, 1, 1, 4, 3, 3, 4, 0, 4, 0, 3, 3, 4, 1,\n",
      "       2, 3, 0, 3, 4, 0, 4, 0, 3, 2, 4, 3, 4, 4, 3, 4, 4, 0, 4, 4],\n",
      "      dtype=int32)>}\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 0 4 0 4 4 4 4 4 4 4 4 4 4 4 4 4 3 4 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "tilapia = Tilapia(lstm_size=5, dense_size=100, output_dim=5, input_shape=(input_len, glove_dim)).build()\n",
    "tilapia.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "log_dir = os.path.join(SRC_DIR, 'logs', 'tilapia_5cell_100dense_{}'.format(datetime.fromtimestamp(time.time()).strftime('%H-%M-%S_%Y-%m-%d')))\n",
    "tb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=False, update_freq='batch')\n",
    "tilapia.summary()\n",
    "tilapia.fit(dataset['train'].take(1000),\n",
    "            epochs=num_epochs,\n",
    "            validation_data = dataset['val'].take(1000),\n",
    "            callbacks = [tb])\n",
    "sanity_check(tilapia, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0606 15:23:45.837588 140323491501888 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f9ed8630320>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Tilapia\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "review (InputLayer)          [(None, 300, 300)]        0         \n",
      "_________________________________________________________________\n",
      "LSTM_1 (UnifiedLSTM)         (None, 5)                 6120      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               600       \n",
      "_________________________________________________________________\n",
      "stars (Dense)                (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 7,225\n",
      "Trainable params: 7,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "   2305/Unknown - 153s 66ms/step - loss: 1.3912 - sparse_categorical_accuracy: 0.4456"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-e386e3aa82ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             callbacks = [tb])\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0msanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtilapia_1000cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    789\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m           initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[0;31m# Case 3: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(generator, mode)\u001b[0m\n\u001b[1;32m    353\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \"\"\"\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    577\u001b[0m             output_shapes=self._flat_output_shapes)\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m_from_compatible_tensor_list\u001b[0;34m(self, flat_value)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0mflat_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstructure\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_nested_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tilapia_1000cell = Tilapia(lstm_size=5, dense_size=100, output_dim=5, input_shape=(input_len, glove_dim)).build()\n",
    "tilapia_1000cell.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "log_dir = os.path.join(SRC_DIR, 'logs', 'tilapia_5cell_100dense_{}'.format(datetime.fromtimestamp(time.time()).strftime('%H-%M-%S_%Y-%m-%d')))\n",
    "tb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=False, update_freq='batch')\n",
    "tilapia_1000cell.summary()\n",
    "tilapia_1000cell.fit(dataset['train'],\n",
    "            epochs=num_epochs,\n",
    "            validation_data = dataset['val'],\n",
    "            callbacks = [tb])\n",
    "sanity_check(tilapia_1000cell, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_model(tilapia, show_shapes=False, show_layer_names=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yelp-nlu_3.6",
   "language": "python",
   "name": "yelp-nlu_3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
