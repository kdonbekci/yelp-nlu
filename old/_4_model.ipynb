{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packages import *\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score, r2_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from collections import Counter\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "We will be building our models in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some configuration required for tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-7\n",
    "fp = 'float32'\n",
    "tf.config.gpu.set_per_process_memory_growth(True)\n",
    "tf.config.gpu.set_per_process_memory_fraction(.2)\n",
    "tf.keras.backend.set_floatx(fp)\n",
    "tf.keras.backend.set_epsilon(epsilon)\n",
    "tf.keras.backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "glove_dim = 50\n",
    "input_shape = (300, glove_dim, 1)\n",
    "max_length = 300\n",
    "shuffle_buffer_size = batch_size*4\n",
    "prefetch_buffer_size = 1\n",
    "random_seed = np.random.randint(0, 100)\n",
    "test_ratio = 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_sample(dictionary, n_sample=None, random_seed=42):\n",
    "    lens = [len(l) for l in dictionary.values()]\n",
    "    assert min(lens) == max(lens)\n",
    "    n_data = lens[0]\n",
    "    processed = {}\n",
    "    for key, array in dictionary.items():\n",
    "        if n_sample is not None:\n",
    "            processed[key] = shuffle(array, random_state=random_seed)[:n_sample]\n",
    "        else:\n",
    "            processed[key] = shuffle(array, random_state=random_seed)[:n_sample]\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = shuffle_sample(load_data({'review': ['text', 'stars']})['review'],\n",
    "                             random_seed=random_seed\n",
    "                            )\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['text'],\n",
    "                                                    data['stars'],\n",
    "                                                    test_size = test_ratio,\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(x_train)\n",
    "num_batches = n // batch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_lookup = load_pickle(os.path.join(GLOVE_DIR, 'glove-{}D.pkl'.format(glove_dim)))\n",
    "unk_key = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_vector = np.mean(np.array(list(glove_lookup.values())), axis=0)\n",
    "glove_lookup[unk_key] = unk_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_text = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "test_dataset_text = tf.data.Dataset.from_tensor_slices(x_test)\n",
    "\n",
    "train_dataset_stars = tf.data.Dataset.from_tensor_slices(y_train - 1) # to make stars 0-indexed\n",
    "test_dataset_stars = tf.data.Dataset.from_tensor_slices(y_test - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_token(t):\n",
    "    t = t.lower()\n",
    "    m = re.match('^[^\\w\\'](\\w+).*', t)\n",
    "    if m is not None:\n",
    "        return m.group(1)\n",
    "    else:\n",
    "        return t\n",
    "\n",
    "def tokenize(s):\n",
    "    tokens = []\n",
    "    for t in word_tokenize(s):\n",
    "        tokens.append(clean_token(t))\n",
    "    return tokens\n",
    "\n",
    "# Returns an np.array of glove embeddings for each\n",
    "# word in the given string of shape (word_count, glove_dims)\n",
    "def get_glove_embeddings(tokens):\n",
    "    embeddings = []\n",
    "    \n",
    "    for i, word in enumerate(tokens):\n",
    "#         if i > \n",
    "        if word in glove_lookup:\n",
    "            embeddings.append(glove_lookup[word])\n",
    "        else:\n",
    "            embeddings.append(np.zeros(glove_dim))\n",
    "            \n",
    "#     return(np.array(embeddings, dtype=np.float16))\n",
    "    return(np.array(embeddings, dtype=np.float32))\n",
    "\n",
    "def test_embed(arr):\n",
    "    return get_glove_embeddings(tokenize(arr))\n",
    "\n",
    "def embed(tensor):\n",
    "    return get_glove_embeddings(tokenize(str(tensor.numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for i in range(128 * 100):\n",
    "#     _=test_embed(x_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def fix_dimensions(tensor):\n",
    "    return tf.image.resize_image_with_crop_or_pad(tf.expand_dims(tensor, axis=-1), 300, glove_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_embed = tf.data.Dataset.map(train_dataset_text,  lambda review: tf.py_function( embed, [review], tf.float32 ), num_parallel_calls=AUTOTUNE) \n",
    "test_dataset_embed = tf.data.Dataset.map(test_dataset_text,  lambda review: tf.py_function( embed, [review], tf.float32 ),num_parallel_calls=AUTOTUNE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_embed = tf.data.Dataset.map(train_dataset_embed, fix_dimensions, num_parallel_calls=AUTOTUNE) \n",
    "test_dataset_embed = tf.data.Dataset.map(test_dataset_embed, fix_dimensions, num_parallel_calls=AUTOTUNE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.zip((train_dataset_embed, train_dataset_stars))\n",
    "test_dataset = tf.data.Dataset.zip((test_dataset_embed, test_dataset_stars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.shuffle(train_dataset, buffer_size=shuffle_buffer_size)\n",
    "test_dataset = tf.data.Dataset.shuffle(test_dataset, buffer_size=shuffle_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.batch(train_dataset, batch_size=batch_size)\n",
    "test_dataset = tf.data.Dataset.batch(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.prefetch(train_dataset, buffer_size=prefetch_buffer_size)\n",
    "test_dataset = tf.data.Dataset.prefetch(test_dataset, buffer_size=prefetch_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-681aae616ea5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \"\"\"\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yelp-nlu_3.6/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m         \u001b[0;34m\"IteratorGetNextSync\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m         \"output_types\", output_types, \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   1941\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for x_batch, y_batch in train_dataset.take(1000 // batch_size):\n",
    "    X.append(x_batch)\n",
    "    Y.append(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2357"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=855, shape=(16,), dtype=int16, numpy=array([4, 4, 0, 4, 2, 4, 4, 4, 1, 4, 3, 2, 0, 2, 4, 4], dtype=int16)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6250"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4479552"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batches * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]],\n",
       "\n",
       "        [[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]],\n",
       "\n",
       "        [[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]],\n",
       "\n",
       "        [[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]],\n",
       "\n",
       "        [[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.08085 ],\n",
       "         [-0.43118 ],\n",
       "         [ 0.15614 ],\n",
       "         ...,\n",
       "         [-0.028197],\n",
       "         [-0.51003 ],\n",
       "         [ 0.61695 ]],\n",
       "\n",
       "        [[ 0.55561 ],\n",
       "         [ 0.1704  ],\n",
       "         [ 0.13692 ],\n",
       "         ...,\n",
       "         [-0.32978 ],\n",
       "         [ 0.24825 ],\n",
       "         [-0.38275 ]],\n",
       "\n",
       "        [[ 0.92871 ],\n",
       "         [-0.10834 ],\n",
       "         [ 0.21497 ],\n",
       "         ...,\n",
       "         [ 0.5558  ],\n",
       "         [-0.70359 ],\n",
       "         [-0.52693 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.013441],\n",
       "         [ 0.23682 ],\n",
       "         [-0.16899 ],\n",
       "         ...,\n",
       "         [-0.56657 ],\n",
       "         [ 0.044691],\n",
       "         [ 0.30392 ]],\n",
       "\n",
       "        [[ 0.52905 ],\n",
       "         [-0.30145 ],\n",
       "         [ 0.056191],\n",
       "         ...,\n",
       "         [ 0.067963],\n",
       "         [-0.10931 ],\n",
       "         [ 0.17161 ]],\n",
       "\n",
       "        [[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]],\n",
       "\n",
       "        [[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]],\n",
       "\n",
       "        [[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]],\n",
       "\n",
       "        [[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]],\n",
       "\n",
       "        [[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]],\n",
       "\n",
       "        [[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]],\n",
       "\n",
       "        [[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]],\n",
       "\n",
       "        [[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]],\n",
       "\n",
       "        [[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]],\n",
       "\n",
       "        [[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]],\n",
       "\n",
       "        [[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]],\n",
       "\n",
       "        [[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]],\n",
       "\n",
       "        [[ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         ...,\n",
       "         [ 0.      ],\n",
       "         [ 0.      ],\n",
       "         [ 0.      ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.032286],\n",
       "         [-0.27071 ],\n",
       "         [ 0.68108 ],\n",
       "         ...,\n",
       "         [-0.56699 ],\n",
       "         [ 0.033216],\n",
       "         [-0.58123 ]],\n",
       "\n",
       "        [[ 0.68047 ],\n",
       "         [-0.039263],\n",
       "         [ 0.30186 ],\n",
       "         ...,\n",
       "         [-0.073297],\n",
       "         [-0.064699],\n",
       "         [-0.26044 ]],\n",
       "\n",
       "        [[ 0.36143 ],\n",
       "         [ 0.58615 ],\n",
       "         [-0.23718 ],\n",
       "         ...,\n",
       "         [ 0.39362 ],\n",
       "         [ 0.36523 ],\n",
       "         [ 0.36077 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.15164 ],\n",
       "         [ 0.30177 ],\n",
       "         [-0.16763 ],\n",
       "         ...,\n",
       "         [-0.35652 ],\n",
       "         [ 0.016413],\n",
       "         [ 0.10216 ]],\n",
       "\n",
       "        [[ 0.418   ],\n",
       "         [ 0.24968 ],\n",
       "         [-0.41242 ],\n",
       "         ...,\n",
       "         [-0.18411 ],\n",
       "         [-0.11514 ],\n",
       "         [-0.78581 ]],\n",
       "\n",
       "        [[ 0.04159 ],\n",
       "         [ 0.21264 ],\n",
       "         [-1.7687  ],\n",
       "         ...,\n",
       "         [ 0.77494 ],\n",
       "         [-0.44315 ],\n",
       "         [ 0.57494 ]]]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel:\n",
    "    def __init__(self):\n",
    "        self.model = LinearSVC()\n",
    "        self.vectorizer = CountVectorizer(max_features = 400000) #number of entries in GLoVe\n",
    "        \n",
    "    def train(self, reviews, stars):\n",
    "        x = self.vectorizer.fit_transform(reviews)\n",
    "        self.model.fit(x, stars)\n",
    "\n",
    "    def predict(self, reviews):\n",
    "        x = self.vectorizer.transform(reviews)\n",
    "        predictions = self.model.predict(x)\n",
    "        return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "monster = BaselineModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monster.train(x_train[:n_train], y_train[:n_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = monster.predict(x_test[:n_train])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "f1_score(y_test[:n_train], predictions, average='macro')\n",
    "0.43700495451651733"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "f1_score(y_test[:n_train], predictions, average='micro')\n",
    "0.612856"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BabyBlueberry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BabyBlueberry:\n",
    "\n",
    "    def build(input_shape = input_shape, star_units=5):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = Conv2D(16, (3, 3), activation='relu', name='block1_conv1', padding='same')(inputs)\n",
    "        x = Conv2D(16, (3, 3), activation='relu', name='block1_conv2', padding='same')(x)\n",
    "        x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "        x = Conv2D(32, (3, 3), activation='relu', name='block2_conv1', padding='same')(x)\n",
    "        x = Conv2D(32, (3, 3), activation='relu', name='block2_conv2', padding='same')(x)\n",
    "        x = Conv2D(32, (3, 3), activation='relu', name='block2_conv3', padding='same')(x)\n",
    "        x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "        x = Conv2D(64, (3, 3), activation='relu', name='block3_conv1', padding='same')(x)\n",
    "        x = Conv2D(64, (3, 3), activation='relu', name='block3_conv2', padding='same')(x)\n",
    "        x = Conv2D(64, (3, 3), activation='relu', name='block3_conv3', padding='same')(x)\n",
    "        x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x) \n",
    "        x = Conv2D(128, (3, 3), activation='relu', name='block4_conv1', padding='same')(x)\n",
    "        x = Conv2D(128, (3, 3), activation='relu', name='block4_conv2', padding='same')(x)\n",
    "        x = Conv2D(128, (3, 3), activation='relu', name='block4_conv3', padding='same')(x)\n",
    "        x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x) \n",
    "        x = Flatten(name='flatten')(x)\n",
    "        x = Dropout(rate=.2, name='dropout')(x)\n",
    "        out = Dense(star_units, activation='softmax', name='output')(x)\n",
    "        return tf.keras.models.Model(inputs=inputs, outputs=out)\n",
    "        \n",
    "    @tf.function\n",
    "    def loss_fn(truth, logits):\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(truth, logits)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.7 s, sys: 19.7 s, total: 1min 16s\n",
      "Wall time: 42.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in train_dataset.take(100):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "baby = BabyBlueberry.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "baby.compile(optimizer=adam, metrics=['accuracy'] ,loss=BabyBlueberry.loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "34997/34997 [==============================] - 14241s 407ms/step - loss: 0.8946 - accuracy: 0.1770\n",
      "Epoch 2/2\n",
      "34997/34997 [==============================] - 14242s 407ms/step - loss: 0.8171 - accuracy: 0.1790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f34fbf2ffd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baby.fit(train_dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 410s 410ms/step - loss: 0.8017 - accuracy: 0.1655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8017106358408927, 0.16546875]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baby.evaluate(test_dataset.take(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 6s 577ms/step - loss: 0.7471 - accuracy: 0.1750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7470987379550934, 0.175]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baby.evaluate(train_dataset.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dists, ix = neighbors(glove_lookup['female'] + glove_lookup['king'], glove_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dists[dists.columns[ix[::-1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yelp-nlu_3.6",
   "language": "python",
   "name": "yelp-nlu_3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
