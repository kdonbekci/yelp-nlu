{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packages import *\n",
    "import tensorflow as tf\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pandas as pd\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils import shuffle\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "We will be building our models in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some configuration required for tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.gpu.set_per_process_memory_growth(True)\n",
    "tf.config.gpu.set_per_process_memory_fraction(.3)\n",
    "tf.keras.backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using random_seed: 937717800\n"
     ]
    }
   ],
   "source": [
    "glove_dim = 50\n",
    "max_length = 300\n",
    "chunk_count = 100\n",
    "random_seed = np.random.randint(0, 1000000000)\n",
    "print('using random_seed: {}'.format(random_seed))\n",
    "splits = ['train', 'val', 'test'] #60:20:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60:20:20)\n",
      "oops 1 times.\n"
     ]
    }
   ],
   "source": [
    "tf_text_filenames = [os.path.join(DATASET_DIR, 'preprocessed', 'tfrecord', 'xext',  #xext temporarily\n",
    "                                  'review-text-{:02d}.tf'.format(i)) for i in range(chunk_count)]\n",
    "tf_ix_filenames = [os.path.join(DATASET_DIR, 'preprocessed', 'tfrecord', 'ix', \n",
    "                                  'review-ix-{:02d}.tf'.format(i)) for i in range(chunk_count)]\n",
    "all_stars = (load_data({'review': ['stars']})['review']['stars'] - 1).astype(np.int32)# this is so that stars are 0-indexed\n",
    "N = {}\n",
    "stars_chunked = all_stars.reshape((chunk_count, 6685900//chunk_count))\n",
    "\n",
    "class_weights = compute_class_weight('balanced', [0, 1, 2, 3, 4], all_stars)\n",
    "class_weights = {'five': class_weights, 'ternary': np.array([class_weights[0], class_weights[1:4].sum() , class_weights[4]])}\n",
    "\n",
    "tf_text, tf_ix, stars= {}, {}, {}\n",
    "\n",
    "tf_text['train'], tf_text_filenames_val_test, tf_ix['train'], tf_ix_filenames_val_test, stars['train'], stars_val_test \\\n",
    "= train_test_split(tf_text_filenames, tf_ix_filenames, stars_chunked, random_state=random_seed, test_size = .4)\n",
    "\n",
    "tf_text['val'], tf_text['test'], tf_ix['val'], tf_ix['test'], stars['val'], stars['test'] \\\n",
    "= train_test_split(tf_text_filenames_val_test, tf_ix_filenames_val_test, stars_val_test, random_state=random_seed, test_size = .5)\n",
    "\n",
    "for split in splits:\n",
    "    stars[split] = stars[split].reshape(np.product(stars[split].shape))\n",
    "    N[split] = len(stars[split])\n",
    "\n",
    "print('({}:{}:{})'.format(len(tf_text['train']), len(tf_text['val']), len(tf_text['test'])))\n",
    "\n",
    "glove_lookup = load_pickle(os.path.join(GLOVE_DIR, 'glove-{}D-byte-float32.pkl'.format(glove_dim)))\n",
    "keys_to_ix = load_pickle(os.path.join(GLOVE_DIR, 'glove-byte-keys_to_ix.pkl'))\n",
    "ix_to_key = {value: key for key, value in keys_to_ix.items()}\n",
    "\n",
    "unk_vector = np.mean(np.array(list(glove_lookup.values())), axis=0)\n",
    "null_vector = np.zeros(glove_dim)\n",
    "glove_lookup[UNK_KEY.encode('ascii')] = unk_vector #if using byte glove dict\n",
    "glove_lookup[NULL_KEY.encode('ascii')] = null_vector\n",
    "\n",
    "oops = 0\n",
    "glove_lookup_array = []\n",
    "for i in range(len(ix_to_key)):\n",
    "    if ix_to_key[i] not in glove_lookup:\n",
    "        oops+=1\n",
    "    glove_lookup_array.append(glove_lookup.get(ix_to_key[i], null_vector))\n",
    "glove_lookup_array = np.array(glove_lookup_array, dtype=np.float32)\n",
    "print('oops {} times.'.format(oops))\n",
    "\n",
    "@tf.function\n",
    "def _parse_function(proto, to_ix):\n",
    "    # define your tfrecord again. Remember that you saved your image as a string.\n",
    "    keys_to_features = {'review': tf.io.FixedLenFeature([300,], tf.int64) if to_ix else tf.io.FixedLenFeature([300,], tf.string),}\n",
    "    \n",
    "    # Load one example\n",
    "    parsed_features = tf.io.parse_single_example(proto, keys_to_features)\n",
    "    \n",
    "    return parsed_features \n",
    "\n",
    "embedding = tf.constant(glove_lookup_array)\n",
    "@tf.function\n",
    "def embed(tensor):\n",
    "    return tf.gather(embedding, tensor)\n",
    "\n",
    "@tf.function\n",
    "def add_channel(tensor):\n",
    "    return tf.expand_dims(tensor, -1)\n",
    "\n",
    "# 11 GB in memory\n",
    "# X = []\n",
    "# x = tf.data.TFRecordDataset(tf_text['train'])\n",
    "# for y in x :\n",
    "#     X.append(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "# def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_review_length(review):\n",
    "    return tf.cast(tf.reduce_sum(tf.cast(tf.not_equal(review, 0), tf.int32)), tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(batch_size, to_embed=False, to_ix=False, channelize=True, to_ternary_task=False, features=None): \n",
    "    '''\n",
    "    Note: once created dataset remains the same from iteration to iteration. \n",
    "    In Keras.fit, once given steps_per_epoch the data for epochs are not the same.\n",
    "    Validation data is the same.\n",
    "    If cell is rerun, then the same order of data will be fed.\n",
    "    '''\n",
    "    if to_ternary_task:\n",
    "        star_mapping = tf.constant([0, 1, 1, 1, 2], dtype=tf.int32)\n",
    "    def _prepare_star(star):\n",
    "        if to_ternary_task: return {'stars': tf.gather(star_mapping, star)}\n",
    "        return {'stars': star}\n",
    "    def _parse_transform(x):\n",
    "        parsed = _parse_function(x, True)['review']\n",
    "        out = {}\n",
    "        if features:\n",
    "            out = {key: f(parsed) for key, f in features.items()}\n",
    "        if to_ix:\n",
    "            out['review-ix'] = tf.cast(parsed, tf.int32)\n",
    "        if to_embed:\n",
    "            if channelize:\n",
    "                out['review-embed'] = add_channel(embed(parsed))\n",
    "            else:\n",
    "                out['review-embed'] = embed(parsed) \n",
    "        return out   \n",
    "    \n",
    "    shuffle_buffer_size = batch_size * 100\n",
    "    prefetch_buffer_size = 4\n",
    "    files = tf_ix\n",
    "    dataset = {}\n",
    "    num_batches = {}\n",
    "    for split in splits:\n",
    "        dataset[split] = tf.data.TFRecordDataset(files[split])\n",
    "        stars_dataset = tf.data.Dataset.from_tensor_slices(stars[split])\n",
    "        dataset[split] = tf.data.Dataset.zip((dataset[split], stars_dataset))\n",
    "        dataset[split] = dataset[split].shuffle(shuffle_buffer_size)\n",
    "        dataset[split] = dataset[split].repeat()\n",
    "        dataset[split] = dataset[split].map(lambda x, y: (_parse_transform(x), _prepare_star(y)), num_parallel_calls=12)\n",
    "        dataset[split] = dataset[split].batch(batch_size)\n",
    "        dataset[split] = dataset[split].prefetch(prefetch_buffer_size)\n",
    "        num_batches[split] = N[split] // batch_size\n",
    "    print('num_batches: {}, batch_size: {}, shuffle_buffer_size: {}, prefetch_buffer_size: {}'.format(num_batches, batch_size, \n",
    "                                                                                 shuffle_buffer_size, prefetch_buffer_size))\n",
    "    return dataset, num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches: {'train': 62680, 'val': 20893, 'test': 20893}, batch_size: 64, shuffle_buffer_size: 6400, prefetch_buffer_size: 4\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "dataset, num_batches = make_dataset(batch_size=batch_size, to_ix=True, features={'review-length': get_review_length})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.11 s, sys: 2.12 s, total: 11.2 s\n",
      "Wall time: 2.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, (x, y) in enumerate(dataset['train']):\n",
    "    if i == 500:\n",
    "        break\n",
    "for i, (x, y) in enumerate(dataset['train']):\n",
    "    if i == 500:\n",
    "        break\n",
    "# Wall time: 30.1 --> 14.2 --> 4.3 s wow (64 batchsize, glovedim = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_init, y_init in dataset['train']:break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BabyBlueberry\n",
    "Convolutional Model with GLoVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@rename('all_acc')\n",
    "def all_class_accuracy(y_true, y_pred):\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred) \n",
    "\n",
    "def one_class_recall(label):\n",
    "    @rename('{}_recalll'.format(label))\n",
    "    def single_class_recall(y_true, y_pred): # (64, 1), (64, 5) should return (64,)\n",
    "        truth = K.flatten(K.cast(y_true, 'int32'))\n",
    "        preds = K.cast(K.argmax(y_pred, axis=-1), 'int32')\n",
    "        recall_mask = K.cast(K.equal(truth, label), 'int32')        \n",
    "        recall = K.cast(K.equal(preds, truth), 'int32') * recall_mask\n",
    "        recall = K.cast(K.sum(recall) / K.maximum(K.sum(recall_mask), 1), 'float32')\n",
    "        return  K.ones(batch_size, dtype='float32') * recall\n",
    "    return single_class_recall\n",
    "\n",
    "def one_class_precision(label):\n",
    "    @rename('{}_precision'.format(label))\n",
    "    def single_class_precision(y_true, y_pred): # (64, 1), (64, 5) should return (64,)\n",
    "        truth = K.flatten(K.cast(y_true, 'int32'))\n",
    "        preds = K.cast(K.argmax(y_pred, axis=-1), 'int32')\n",
    "        precision_mask = K.cast(K.equal(preds, label), 'int32')          \n",
    "        precision = K.cast(K.equal(preds, truth), 'int32') * precision_mask\n",
    "        precision = K.cast(K.sum(precision) / K.maximum(K.sum(precision_mask), 1), 'float32')\n",
    "        return  K.ones(batch_size, dtype='float32') * precision\n",
    "    return single_class_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_metrics = [all_class_accuracy]\n",
    "star_metrics += [one_class_recall(i) for i in range(5)] + [one_class_precision(i) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernel_spec(filters, height, width):\n",
    "    return {'filters':filters, 'height': height, 'width': width}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BabyBerry:\n",
    "    \n",
    "    def build(dropout_rate, out_units):\n",
    "        ix_input = Input(shape= (max_length,), name='review-ix')\n",
    "        glove_input = Input(shape=(max_length, glove_dim), name='review-embed')\n",
    "        \n",
    "        \n",
    "        embedding = Embedding(input_dim=len(glove_lookup_array), output_dim=glove_dim, \n",
    "                              input_length=max_length, name='trained_embed')(ix_input)\n",
    "                        \n",
    "        average_embedding = Average(name='average_embed')([embedding, glove_input])\n",
    "        \n",
    "        \n",
    "        x = LSTM(units=500, go_backwards=True, name='lstm')(average_embedding)\n",
    "        x = Dropout(rate=dropout_rate, name='dropout_1')(x)\n",
    "        x = Dense(100, activation='relu', name='dense_1')(x)\n",
    "        out = Dense(out_units, activation='softmax', name='stars')(x)\n",
    "        \n",
    "        return tf.keras.models.Model(inputs={'review-embed':glove_input, 'review-ix': ix_input}, \n",
    "                                     outputs={'stars': out}, name='BabyBerry')\n",
    "        \n",
    "    @tf.function\n",
    "    def loss_fn(truth, logits):\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(truth, logits)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "dataset, num_batches = make_dataset(batch_size=batch_size, to_embed=True, to_ix=True, channelize=False, to_ternary_task=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baby = BabyBerry.build(dropout_rate=.1, out_units=5)\n",
    "adam = tf.keras.optimizers.Adam()\n",
    "baby.compile(optimizer=adam, metrics={'stars': star_metrics} ,loss={'stars': BabyBerry.loss_fn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(baby, show_shapes=True, rankdir='LR', to_file='images/lstm_baby_berry_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(SRC_DIR, 'logs', '{}-LSTM-BabyBerry'.format(datetime.fromtimestamp(time.time()).strftime('%d-%m_%H-%M-%S')))\n",
    "tb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, write_graph=False, update_freq='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=baby.fit(dataset['train'], steps_per_epoch=200, epochs=50, \n",
    "                             validation_data= dataset['val'], validation_steps=20,\n",
    "                             callbacks=[tb], class_weight=class_weights['five'], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth, predictions = [], []\n",
    "for x, y in dataset['val'].take(100):\n",
    "    pred = baby(x)\n",
    "    truth.append(y['stars'].numpy())\n",
    "    predictions.append(tf.argmax(pred['stars'], axis=-1).numpy())\n",
    "predictions=np.array(predictions).flatten()\n",
    "truth = np.array(truth).flatten()\n",
    "print(classification_report(truth, predictions))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baby_embedding = baby.layers[1].get_weights()[0]\n",
    "baby_dict = {key: baby_embedding[val] for key, val in keys_to_ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists, ix = neighbors(baby_dict['money'.encode()], baby_dict)\n",
    "dists[dists.columns[ix]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeenBerry:\n",
    "    \n",
    "    def build(dropout_rate, lstm_units, kernels, out_units, embedding_array):\n",
    "        ix_input = Input(shape=(max_length,), name='review-ix')\n",
    "        \n",
    "        embedding = Embedding(input_dim=len(glove_lookup_array), output_dim=glove_dim, \n",
    "                              input_length=max_length, name='trained_embed', weights=[embedding_array])(ix_input)\n",
    "        channel_embedding = Reshape(target_shape=(max_length, glove_dim, 1), name='reshape_embed')(embedding)\n",
    "        \n",
    "        branches = []\n",
    "        names = {}\n",
    "        for kernel in kernels:\n",
    "            if (kernel['height'], kernel['width']) in names:\n",
    "                names[(kernel['height'], kernel['width'])] = i = names[(kernel['height'], kernel['width'])] + 1\n",
    "            else:\n",
    "                names[(kernel['height'], kernel['width'])] = i = 1\n",
    "            branch = Conv2D(kernel['filters'], (kernel['height'], kernel['width']), activation='relu', \n",
    "                            name='conv_{}_{}_{}'.format(kernel['height'], kernel['width'], i), \n",
    "                            padding='valid')(channel_embedding)\n",
    "            branch = GlobalMaxPool2D(name='pool_{}_{}_{}'.format(kernel['height'], kernel['width'], i))(branch)\n",
    "            branches.append(branch)\n",
    "        \n",
    "        lstm_out = LSTM(units=lstm_units, name='lstm')(embedding)\n",
    "        x = Concatenate(name='concat_conv')(branches)\n",
    "        \n",
    "        x = Dropout(rate=dropout_rate, name='dropout')(x)\n",
    "        x = Concatenate(name='concat_conv_lstm')([x, lstm_out])\n",
    "        x = Dense(500, activation='relu', name='dense')(x)\n",
    "        out = Dense(out_units, activation='softmax', name='stars')(x)\n",
    "        \n",
    "        return tf.keras.models.Model(inputs={'review-ix': ix_input}, \n",
    "                                     outputs={'stars': out}, name='TeenBerry')\n",
    "        \n",
    "    @tf.function\n",
    "    def loss_fn(truth, logits):\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(truth, logits)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teen = TeenBerry.build(kernels=kernels, dropout_rate=.3, out_units=3, lstm_units=600, embedding_array=learned_embedding)\n",
    "adam = tf.keras.optimizers.Adam()\n",
    "teen.compile(optimizer=adam, metrics={'stars': star_metrics} ,loss={'stars': TeenBerry.loss_fn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(teen, show_shapes=True, rankdir='LR', to_file='teen_berry_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(SRC_DIR, 'logs', 'TeenBerry-{}'.format(datetime.fromtimestamp(time.time()).strftime('%d-%m_%H-%M-%S')))\n",
    "tb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, write_graph=False, update_freq='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=teen.fit(dataset['train'], steps_per_epoch=200, epochs=50, \n",
    "                             validation_data= dataset['val'], validation_steps=20,\n",
    "                             callbacks=[tb], class_weight=class_weights['ternary'], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth, predictions = [], []\n",
    "for x, y in dataset['val'].take(100):\n",
    "    pred = teen(x['review-ix'])\n",
    "    truth.append(y['stars'].numpy())\n",
    "    predictions.append(tf.argmax(pred['stars'], axis=-1).numpy())\n",
    "predictions=np.array(predictions).flatten()\n",
    "truth = np.array(truth).flatten()\n",
    "print(classification_report(truth, predictions))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.83      0.77      0.80      1833\n",
    "           1       0.77      0.77      0.77      5373\n",
    "           2       0.83      0.84      0.84      5594\n",
    "\n",
    "    accuracy                           0.80     12800\n",
    "   macro avg       0.81      0.80      0.80     12800\n",
    "weighted avg       0.80      0.80      0.80     12800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teen_embedding = teen.layers[1].get_weights()[0]\n",
    "teen_dict = {key: teen_embedding[val] for key, val in keys_to_ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teen_dists, teen_ix = neighbors(teen_dict['pizza'.encode()], teen_dict)\n",
    "teen_dists[teen_dists.columns[teen_ix]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatureBerry:\n",
    "    \n",
    "    def build(dropout_rate, lstm_units, kernels, out_units, embedding_array):\n",
    "        ix_input = Input(shape=(max_length,), name='review-ix')\n",
    "        \n",
    "        embedding = Embedding(input_dim=len(glove_lookup_array), output_dim=glove_dim, \n",
    "                              input_length=max_length, name='trained_embed', weights=[embedding_array], trainable=False)(ix_input)\n",
    "        \n",
    "        channel_embedding = Reshape(target_shape=(max_length, glove_dim, 1), name='reshape_embed')(embedding)\n",
    "        \n",
    "        branches = []\n",
    "        names = {}\n",
    "        for kernel in kernels:\n",
    "            if (kernel['height'], kernel['width']) in names:\n",
    "                names[(kernel['height'], kernel['width'])] = i = names[(kernel['height'], kernel['width'])] + 1\n",
    "            else:\n",
    "                names[(kernel['height'], kernel['width'])] = i = 1\n",
    "            branch = Conv2D(kernel['filters'], (kernel['height'], kernel['width']), activation='relu', \n",
    "                            name='conv_{}_{}_{}'.format(kernel['height'], kernel['width'], i), \n",
    "                            padding='valid')(channel_embedding)\n",
    "            branch = GlobalMaxPool2D(name='pool_{}_{}_{}'.format(kernel['height'], kernel['width'], i))(branch)\n",
    "            branches.append(branch)\n",
    "        \n",
    "#         lstm_out = LSTM(units=lstm_units, name='lstm')(embedding)\n",
    "        x = Concatenate(name='concat_conv')(branches)\n",
    "        \n",
    "        x = Dropout(rate=dropout_rate, name='dropout')(x)\n",
    "#         x = Concatenate(name='concat_conv_lstm')([x, lstm_out])\n",
    "        x = Dense(500, activation='relu', name='dense')(x)\n",
    "        out = Dense(out_units, activation='softmax', name='stars')(x)\n",
    "        \n",
    "        return tf.keras.models.Model(inputs={'review-ix': ix_input}, \n",
    "                                     outputs={'stars': out}, name='MatureBerry')\n",
    "        \n",
    "    @tf.function\n",
    "    def loss_fn(truth, logits):\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(truth, logits)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mature = MatureBerry.build(kernels=kernels, dropout_rate=.2, out_units=5, lstm_units=600, embedding_array=teen_embedding)\n",
    "adam = tf.keras.optimizers.Adam()\n",
    "mature.compile(optimizer=adam, metrics={'stars': star_metrics} ,loss={'stars': MatureBerry.loss_fn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(mature, show_shapes=True, rankdir='LR', to_file='mature_berry_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(SRC_DIR, 'logs', 'MatureBerry-{}'.format(datetime.fromtimestamp(time.time()).strftime('%d-%m_%H-%M-%S')))\n",
    "tb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, write_graph=False, update_freq='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=mature.fit(dataset['train'], steps_per_epoch=200, epochs=50, \n",
    "                             validation_data= dataset['val'], validation_steps=20,\n",
    "                             callbacks=[tb], class_weight=class_weights['five'], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth, predictions = [], []\n",
    "for x, y in dataset['val'].take(100):\n",
    "    pred = mature(x['review-ix'])\n",
    "    truth.append(y['stars'].numpy())\n",
    "    predictions.append(tf.argmax(pred['stars'], axis=-1).numpy())\n",
    "predictions=np.array(predictions).flatten()\n",
    "truth = np.array(truth).flatten()\n",
    "print(classification_report(truth, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mature_embedding = mature.layers[1].get_weights()[0]\n",
    "mature_dict = {key: mature_embedding[val] for key, val in keys_to_ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mature_dists, mature_ix = neighbors(mature_dict['loved'.encode()], mature_dict)\n",
    "mature_dists[mature_dists.columns[mature_ix]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    histories\n",
    "except:\n",
    "    histories, reports = { }, { }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_heights = list(np.arange(5, step=1)+2) + list(np.arange(10, 50, step=5)) #+ list(np.arange(50, 101, step=10))\n",
    "possible_num_branches = list(np.arange(4, 11, step=2))\n",
    "possible_filters = list(np.arange(200, 501, step=100))\n",
    "possible_dropouts = list(np.arange(.1, .51, step=.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('possible_heights: {}'.format(possible_heights))\n",
    "print('possible_num_branches: {}'.format(possible_num_branches))\n",
    "print('possible_filters: {}'.format(possible_filters))\n",
    "print('possible_dropouts: {}'.format(possible_dropouts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_report_histories():\n",
    "    save_pickle('histories.pkl', reports)\n",
    "    save_pickle('reports.pkl', reports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_report_histories():\n",
    "    return (load_pickle('histories.pkl'),\n",
    "            load_pickle('reports.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_heights = set(possible_heights) #+ list(np.arange(50, 101, step=10)))\n",
    "possible_num_branches = shuffle(possible_num_branches)\n",
    "possible_filters = shuffle(possible_filters)\n",
    "possible_dropouts = shuffle(possible_dropouts)\n",
    "histories, reports = load_report_histories()\n",
    "i = 0\n",
    "for num_branches in possible_num_branches:\n",
    "    subsets = find_subsets_of_n(possible_heights, num_branches)\n",
    "    subsets = shuffle(subsets)\n",
    "    for heights in subsets:\n",
    "        for filters in possible_filters:\n",
    "            for dropout_rate in possible_dropouts:\n",
    "                \n",
    "                params = (tuple(heights), filters, dropout_rate)\n",
    "                if params in reports:\n",
    "                    continue\n",
    "                print(params)\n",
    "                kernels = [get_kernel_spec(filters=filters, height=h, width=glove_dim) for h in heights]\n",
    "                baby = BabyBlueberry.build(kernels=kernels, dropout_rate=dropout_rate)\n",
    "                adam = tf.keras.optimizers.Adam()\n",
    "                baby.compile(optimizer=adam, metrics={'stars': star_metrics} ,loss={'stars': BabyBlueberry.loss_fn})\n",
    "                log_dir = os.path.join(SRC_DIR, 'logs', 'CONV-{}'.format(datetime.fromtimestamp(time.time()).strftime('%H-%M-%S_%m-%d')))\n",
    "                tb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, write_graph=False, update_freq='batch')\n",
    "                histories[params] = baby.fit(dataset['train'], steps_per_epoch=100, epochs=5, \n",
    "                                             validation_data= dataset['val'], validation_steps=10,\n",
    "                                             callbacks=[tb], class_weight=class_weights, verbose=1)\n",
    "                predictions, truth = [], []\n",
    "                for x, y in dataset['val'].take(100):\n",
    "                    pred = baby.predict_on_batch(x)\n",
    "                    truth.append(y['stars'].numpy())\n",
    "                    predictions.append(tf.argmax(pred['stars'], axis=-1).numpy())\n",
    "                predictions=np.array(predictions).flatten()\n",
    "                truth = np.array(truth).flatten()\n",
    "                reports[params] = classification_report(truth, predictions)\n",
    "                print(reports[params])\n",
    "                save_report_histories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle('reports.pkl', reports)\n",
    "save_pickle('histories.pkl', histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'].shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in dataset['train']:break\n",
    "truth = y['stars']\n",
    "pred = baby(x['review'])['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(truth, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yelp-nlu_3.6",
   "language": "python",
   "name": "yelp-nlu_3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
