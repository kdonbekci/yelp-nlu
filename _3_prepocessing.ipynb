{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We decided to preprocess the dataset once prior to training to enhance the quality of our data and have it in a ready-to-train format. This included things such as getting rid of HTML markups, replacing accented letters such as à/á/â with their counterparts, and replacing individual words with indices. Finally we serialized and exported the dataset in Tensorflow's binary file format .tfrecords for ease of usability during training. \n",
    "\n",
    "## Table of Contents\n",
    "1. [Preprocessing](#Preprocessing)\n",
    "2. [Serializing and exporting](#Serializing-and-exporting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packages import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import unicodedata\n",
    "import pickle\n",
    "import concurrent.futures\n",
    "import tensorflow as tf\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Wow. So surprised at the one and two star reviews!  We started with the most tender calamari. Although the marinara sauce was a bit bland, but a touch of salt made it just right. My husband had the veal with peppers and said it was so delicious and tender. The mashed potatoes were perfect. I had the salmon Diablo which was also delicious. Our salad was beautiful! Dressing was served on the salad and it was a nice amount. We ended our delicious meal with a piece of tiramisu. Our server Matt was right on!! Very pleasant and knowledgeable about the menu. Our appetizer, salad and entrees were timed perfectly. I love salad and did not mind that my entree was served while I was still eating it! No problem it let my dinner cool to just the right temp for me to eat it comfortably. \\nI wonder sometimes if people just don't appreciate relaxing and taking time to eat a wonderful and beautifully prepared meal.  A wonderful atmosphere. So relaxing. The chairs are super comfortable too!!! We will certainly be back. \\nGive it a try.  Don't  always go by the reviews. \\nA bottle of Riesling, calamari app, two delicious entrees and dessert for $92! \\nWell with it.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_dim = 50\n",
    "tokens_to_keep = set( load_pickle(os.path.join(GLOVE_DIR, 'glove-{}D.pkl'.format(glove_dim))).keys())\n",
    "data = load_data({'review': ['text']})['review']['text']\n",
    "review[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_lengths = [(len(token), token) for token in tokens_to_keep]\n",
    "# token_lengths.sort()\n",
    "# token_lengths = token_lengths[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token_length = 16\n",
    "review_length = 300\n",
    "dtype = '|S{}'.format(max_token_length) #itemsize = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NULL_KEY in tokens_to_keep, UNK_KEY in tokens_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_token(t):\n",
    "    t = unicodedata.normalize('NFKD', t).encode('ascii', 'ignore').decode() #sometimes len() increases after normalization\n",
    "    if len(t) > max_token_length:\n",
    "        return UNK_KEY\n",
    "    t = t.lower()\n",
    "    if t not in tokens_to_keep:\n",
    "        return UNK_KEY\n",
    "    m = re.match('^[^\\w\\'](\\w+).*', t)\n",
    "    if m is not None:\n",
    "        t = m.group(1)\n",
    "    return t\n",
    "\n",
    "def upsample(tokens):\n",
    "    # upsampling strategy is to pad with NULL_KEY's after the last token.\n",
    "    shortage = review_length - len(tokens)\n",
    "    tokens += [NULL_KEY] * shortage\n",
    "    return tokens\n",
    "\n",
    "def downsample(tokens):\n",
    "    # dowsampling strategy is to remove all UNK_KEY's before touching other tokens\n",
    "    excess = len(tokens) - review_length\n",
    "    downsampled_tokens = []\n",
    "    for i, t in enumerate(tokens):\n",
    "        if excess == 0:\n",
    "            downsampled_tokens += tokens[i:]\n",
    "            return downsampled_tokens\n",
    "        if t == UNK_KEY:\n",
    "            excess-=1\n",
    "        else:\n",
    "            downsampled_tokens.append(t)\n",
    "    if excess == 0:\n",
    "        downsampled_tokens += tokens[i+1:]\n",
    "        return downsampled_tokens       \n",
    "    # if we reach here, it means previous attempt didn't get rid of all of the excess, and we will truncate both ends\n",
    "    beg = excess // 2\n",
    "    end = excess - beg\n",
    "    downsampled_tokens = downsampled_tokens[beg:-end]\n",
    "    return downsampled_tokens\n",
    "\n",
    "def tokenize(review):\n",
    "    tokens = [clean_token(t) for t in word_tokenize(review)]\n",
    "    if len(tokens) < review_length:\n",
    "        tokens = upsample(tokens)\n",
    "    elif len(tokens) > review_length:\n",
    "        tokens = downsample(tokens)\n",
    "    return np.array(tokens, dtype=dtype)\n",
    "\n",
    "def preprocess(review):\n",
    "    return tokenize(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed = []\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=10) as executor:\n",
    "    data_preprocessed = executor.map(preprocess, data)\n",
    "data_preprocessed = [i for i in data_preprocessed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(data_preprocessed)\n",
    "splits = 100\n",
    "per_split = N // splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(splits):\n",
    "    x = np.array(data_preprocessed[s * per_split: (s+1) * per_split])\n",
    "    np.save(os.path.join(DATASET_DIR, 'preprocessed', 'review-text-{:02d}.npy'.format(s)), x,\n",
    "            allow_pickle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serializing and exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-7\n",
    "fp = 'float32'\n",
    "tf.config.gpu.set_per_process_memory_growth(True)\n",
    "tf.config.gpu.set_per_process_memory_fraction(.05)\n",
    "tf.keras.backend.set_floatx(fp)\n",
    "tf.keras.backend.set_epsilon(epsilon)\n",
    "tf.keras.backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_ix = True\n",
    "if to_ix:\n",
    "    all_key_to_ix = load_pickle(os.path.join(GLOVE_DIR, 'glove-byte-float32_to_ix.pkl'))\n",
    "    j = [2]\n",
    "    keys_to_ix = {}\n",
    "    keys_to_ix[NULL_KEY.encode()] = 0\n",
    "    keys_to_ix[UNK_KEY.encode()] = 1 \n",
    "#ASSUMING ix 0 is for NULL and 1 for UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helperfunctions to make your feature definition more readable\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[*value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[*value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(review, to_ix):\n",
    "    \"\"\"\n",
    "    Creates a tf.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    # Create a dictionary mapping the feature name to the tf.Example-compatible\n",
    "    # data type.\n",
    "    \n",
    "    \n",
    "    \n",
    "    feature = {\n",
    "      'review': _int64_feature(review) if to_ix else _bytes_feature(review),\n",
    "    }\n",
    "\n",
    "    # Create a Features message using tf.train.Example.\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_count = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_ix(review):\n",
    "    ix = []\n",
    "    for word in review:\n",
    "        if word in keys_to_ix:\n",
    "            ix.append(keys_to_ix[word])\n",
    "        else:\n",
    "            keys_to_ix[word] = j[0]\n",
    "            ix.append(j[0])\n",
    "            j[0]+=1\n",
    "    return ix   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _serialize(review, to_ix=False):\n",
    "    if to_ix:\n",
    "        review = _to_ix(review)\n",
    "    return serialize_example(review, to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_paths = [os.path.join(DATASET_DIR, 'preprocessed', 'npy', 'review-text-{:02d}.npy'.format(i)) for i in range(chunk_count)] #100 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': <tf.Tensor: id=4, shape=(300,), dtype=int64, numpy=\n",
       " array([  2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
       "         15,  16,   7,  17,  18,  19,   3,  20,   7,  21,  22,  23,  24,\n",
       "         25,  26,  27,  28,  24,  29,  30,  31,  32,  33,  34,  35,   3,\n",
       "         36,  37,  38,   7,  39,  16,  40,   9,  41,  33,  23,   4,  42,\n",
       "          9,  18,   3,   7,  43,  44,  45,  46,   3,  47,  38,   7,  48,\n",
       "         49,  50,  23,  51,  42,   3,  52,  53,  23,  54,  13,  55,  23,\n",
       "         56,  57,   7,  53,   9,  33,  23,  24,  58,  59,   3,  14,  60,\n",
       "         52,  42,  61,  16,  24,  62,  30,  63,   3,  52,  64,  65,  23,\n",
       "         35,  57,  13,  13,  66,  67,   9,  68,  69,   7,  70,   3,  52,\n",
       "         71,  27,  53,   9,  72,  45,  73,  74,   3,  47,  75,  53,   9,\n",
       "         76,  77,  78,  79,  36,  80,  23,  56,  81,  47,  23,  82,  83,\n",
       "         33,  13,  84,  85,  33,  86,  36,  87,  88,  89,  34,   7,  35,\n",
       "         90,  91,  92,  89,  93,  33,  94,   3,  47,  95,  96,  97,  98,\n",
       "         34,  99, 100, 101, 102,   9, 103, 104,  89,  93,  24, 105,   9,\n",
       "        106, 107,  61,   3,  24, 105, 108,   3,   4, 102,   3,   7, 109,\n",
       "        110, 111, 112, 113,  13,  13,  13,  14, 114, 115, 116, 117,   3,\n",
       "        118,  33,  24, 119,   3,  99, 100, 120, 121, 122,   7,  12,   3,\n",
       "         24, 123,  30, 124,  27,  19, 125,  27,  10,  42,  72,   9, 126,\n",
       "         91, 127, 128,  13, 129,  16,  33,   3,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0])>}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_parse_function(_serialize(review[12], to_ix=to_ix), to_ix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(chunk_count):\n",
    "    chunk = np.load(review_paths[i],allow_pickle=False)\n",
    "    break\n",
    "    chunk_tf = [_serialize(x, to_ix = to_ix) for x in chunk]\n",
    "    chunk_ds = tf.data.Dataset.from_tensor_slices(chunk_tf)\n",
    "    writer_path = os.path.join(DATASET_DIR, 'preprocessed', 'tfrecord', 'ix' if to_ix else 'xext',\n",
    "                               'review-{}-{:02d}.tf'.format('ix' if to_ix else 'text', i))\n",
    "    writer = tf.data.experimental.TFRecordWriter(writer_path)\n",
    "    writer.write(chunk_ds)\n",
    "    print(i+1, end=', ')\n",
    "if to_ix:\n",
    "    save_pickle(os.path.join(GLOVE_DIR, 'glove-byte-keys_to_ix.pkl'), keys_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(proto, to_ix):\n",
    "    # define your tfrecord again. Remember that you saved your image as a string.\n",
    "    keys_to_features = {'review': tf.io.FixedLenFeature([300,], tf.int64) if to_ix else tf.io.FixedLenFeature([300,], tf.string),}\n",
    "    \n",
    "    # Load one example\n",
    "    parsed_features = tf.io.parse_single_example(proto, keys_to_features)\n",
    "    \n",
    "    return parsed_features "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yelp-nlu_3.6",
   "language": "python",
   "name": "yelp-nlu_3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
